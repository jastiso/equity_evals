{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/stiso/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/stiso/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/stiso/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer as wnl\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from afinn import Afinn\n",
    "import nltk, string, glob\n",
    "import gensim\n",
    "import itertools\n",
    "import re\n",
    "import csv\n",
    "import scipy\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exts = ['bio', 'wiki']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "afn = Afinn()\n",
    "afn.score('bit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bio = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    \"../data/bioASQmodel.txt\", \n",
    "    binary=False, \n",
    "    unicode_errors='ignore'\n",
    ")\n",
    "wiki = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    \"../data/enwiki_20180420_300d.txt\", \n",
    "    binary=False, \n",
    "    unicode_errors='ignore'\n",
    ")\n",
    "\n",
    "models = [bio, wiki]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m.most_similar(positive='woman', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25159326\n",
      "0.23208015\n"
     ]
    }
   ],
   "source": [
    "# most similar words to 'woman' and 'man'\n",
    "word = 'more'\n",
    "print(models[1].similarity(word, 'man'))\n",
    "print(models[1].similarity(word, 'woman'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "# Initialize, config & define helpful functions #\n",
    "#################################################\n",
    "\n",
    "translator = str.maketrans('', '', string.punctuation.replace('-', '')) #filters punctuation except dash\n",
    "lemmatizeCondition = 1\n",
    "lemmatizer = wnl()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Function for finding index of words of interest, like 'references'\n",
    "\n",
    "def find(target):\n",
    "    for i, word in enumerate(sents):\n",
    "        try:\n",
    "            j = word.index(target)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        yield i\n",
    "\n",
    "# Function for handling the input for gensim word2vec\n",
    "\n",
    "class FileToSent(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in open(self.filename, 'r'):\n",
    "            ll = line.strip().split(\",\")\n",
    "            ll = [''.join(c for c in s if c not in string.punctuation) for s in ll]\n",
    "            ll = [num.strip() for num in ll]\n",
    "            yield ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stiso/anaconda2/envs/psom/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3444: DtypeWarning: Columns (11,12,13,69,70) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Evaluatee</th>\n",
       "      <th>StudentName</th>\n",
       "      <th>FormID</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Question</th>\n",
       "      <th>Cid</th>\n",
       "      <th>S_URM</th>\n",
       "      <th>S_SRS.Calc.Race.Desc</th>\n",
       "      <th>Schedule.Start.Date</th>\n",
       "      <th>Answer.Text_Detail</th>\n",
       "      <th>...</th>\n",
       "      <th>F_nh_black_std</th>\n",
       "      <th>F_nh_black_lb</th>\n",
       "      <th>F_nh_black_ub</th>\n",
       "      <th>F_nh_white_mean</th>\n",
       "      <th>F_nh_white_std</th>\n",
       "      <th>F_nh_white_lb</th>\n",
       "      <th>F_nh_white_ub</th>\n",
       "      <th>F_race</th>\n",
       "      <th>F_gender50</th>\n",
       "      <th>F_gender70</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAKHUS, ERIN</td>\n",
       "      <td>HERNANDO, MARY</td>\n",
       "      <td>e2d1755825d32c6be44b8bcf098d20f935a0a665c99a23...</td>\n",
       "      <td></td>\n",
       "      <td>Areas for improvement/Additional comments</td>\n",
       "      <td>MED</td>\n",
       "      <td>Y</td>\n",
       "      <td>Multiple ethnicities selected</td>\n",
       "      <td>9/11/2017</td>\n",
       "      <td>No concerns for her level of training. I only ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044043</td>\n",
       "      <td>0.002035</td>\n",
       "      <td>0.002561</td>\n",
       "      <td>0.931659</td>\n",
       "      <td>0.046282</td>\n",
       "      <td>0.733805</td>\n",
       "      <td>0.747535</td>\n",
       "      <td>nh_white</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAKHUS, ERIN</td>\n",
       "      <td>HERNANDO, MARY</td>\n",
       "      <td>e2d1755825d32c6be44b8bcf098d20f935a0a665c99a23...</td>\n",
       "      <td></td>\n",
       "      <td>Strengths/Summary</td>\n",
       "      <td>MED</td>\n",
       "      <td>Y</td>\n",
       "      <td>Multiple ethnicities selected</td>\n",
       "      <td>9/11/2017</td>\n",
       "      <td>Mary Ann impressed me with her thorough histor...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044043</td>\n",
       "      <td>0.002035</td>\n",
       "      <td>0.002561</td>\n",
       "      <td>0.931659</td>\n",
       "      <td>0.046282</td>\n",
       "      <td>0.733805</td>\n",
       "      <td>0.747535</td>\n",
       "      <td>nh_white</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAMODT, WHITLEY</td>\n",
       "      <td>MEZOCHOW, GABRIELLE</td>\n",
       "      <td>bf3434488ad0a3be485d804cdba2d6aeee56c226519712...</td>\n",
       "      <td></td>\n",
       "      <td>Strengths/Summary</td>\n",
       "      <td>NEU</td>\n",
       "      <td>N</td>\n",
       "      <td>White, Non-Hispanic</td>\n",
       "      <td>3/26/2018</td>\n",
       "      <td>Gabby did an excellent job on the neurology se...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164404</td>\n",
       "      <td>0.161161</td>\n",
       "      <td>0.179011</td>\n",
       "      <td>0.160745</td>\n",
       "      <td>0.111224</td>\n",
       "      <td>0.020521</td>\n",
       "      <td>0.021742</td>\n",
       "      <td>nh_black</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAMODT, WHITLEY</td>\n",
       "      <td>RECH, ANDREW</td>\n",
       "      <td>43cf5ed1d5f3a7ead23e25da8a7d5b0bee2540c12cdf3b...</td>\n",
       "      <td></td>\n",
       "      <td>Strengths/Summary</td>\n",
       "      <td>NEU</td>\n",
       "      <td>N</td>\n",
       "      <td>White, Non-Hispanic</td>\n",
       "      <td>2/26/2018</td>\n",
       "      <td>Andrew did an outstanding job on the PPMC neur...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164404</td>\n",
       "      <td>0.161161</td>\n",
       "      <td>0.179011</td>\n",
       "      <td>0.160745</td>\n",
       "      <td>0.111224</td>\n",
       "      <td>0.020521</td>\n",
       "      <td>0.021742</td>\n",
       "      <td>nh_black</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAMODT, WHITLEY</td>\n",
       "      <td>O''KEEFE, RYAN</td>\n",
       "      <td>1f637c27634cc0b947be3dfcf9998f50fb47d0185e71a4...</td>\n",
       "      <td></td>\n",
       "      <td>Strengths/Summary</td>\n",
       "      <td>NEU</td>\n",
       "      <td>N</td>\n",
       "      <td>White, Non-Hispanic</td>\n",
       "      <td>9/24/2018</td>\n",
       "      <td>Ryan did an excellent job on the HUP neurology...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164404</td>\n",
       "      <td>0.161161</td>\n",
       "      <td>0.179011</td>\n",
       "      <td>0.160745</td>\n",
       "      <td>0.111224</td>\n",
       "      <td>0.020521</td>\n",
       "      <td>0.021742</td>\n",
       "      <td>nh_black</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Evaluatee          StudentName  \\\n",
       "0     AAKHUS, ERIN       HERNANDO, MARY   \n",
       "1     AAKHUS, ERIN       HERNANDO, MARY   \n",
       "2  AAMODT, WHITLEY  MEZOCHOW, GABRIELLE   \n",
       "3  AAMODT, WHITLEY         RECH, ANDREW   \n",
       "4  AAMODT, WHITLEY       O''KEEFE, RYAN   \n",
       "\n",
       "                                              FormID Summary  \\\n",
       "0  e2d1755825d32c6be44b8bcf098d20f935a0a665c99a23...           \n",
       "1  e2d1755825d32c6be44b8bcf098d20f935a0a665c99a23...           \n",
       "2  bf3434488ad0a3be485d804cdba2d6aeee56c226519712...           \n",
       "3  43cf5ed1d5f3a7ead23e25da8a7d5b0bee2540c12cdf3b...           \n",
       "4  1f637c27634cc0b947be3dfcf9998f50fb47d0185e71a4...           \n",
       "\n",
       "                                    Question  Cid S_URM  \\\n",
       "0  Areas for improvement/Additional comments  MED     Y   \n",
       "1                          Strengths/Summary  MED     Y   \n",
       "2                          Strengths/Summary  NEU     N   \n",
       "3                          Strengths/Summary  NEU     N   \n",
       "4                          Strengths/Summary  NEU     N   \n",
       "\n",
       "            S_SRS.Calc.Race.Desc Schedule.Start.Date  \\\n",
       "0  Multiple ethnicities selected           9/11/2017   \n",
       "1  Multiple ethnicities selected           9/11/2017   \n",
       "2            White, Non-Hispanic           3/26/2018   \n",
       "3            White, Non-Hispanic           2/26/2018   \n",
       "4            White, Non-Hispanic           9/24/2018   \n",
       "\n",
       "                                  Answer.Text_Detail  ...  F_nh_black_std  \\\n",
       "0  No concerns for her level of training. I only ...  ...        0.044043   \n",
       "1  Mary Ann impressed me with her thorough histor...  ...        0.044043   \n",
       "2  Gabby did an excellent job on the neurology se...  ...        0.164404   \n",
       "3  Andrew did an outstanding job on the PPMC neur...  ...        0.164404   \n",
       "4  Ryan did an excellent job on the HUP neurology...  ...        0.164404   \n",
       "\n",
       "  F_nh_black_lb F_nh_black_ub F_nh_white_mean  F_nh_white_std  F_nh_white_lb  \\\n",
       "0      0.002035      0.002561        0.931659        0.046282       0.733805   \n",
       "1      0.002035      0.002561        0.931659        0.046282       0.733805   \n",
       "2      0.161161      0.179011        0.160745        0.111224       0.020521   \n",
       "3      0.161161      0.179011        0.160745        0.111224       0.020521   \n",
       "4      0.161161      0.179011        0.160745        0.111224       0.020521   \n",
       "\n",
       "   F_nh_white_ub    F_race  F_gender50  F_gender70  \n",
       "0       0.747535  nh_white           M           M  \n",
       "1       0.747535  nh_white           M           M  \n",
       "2       0.021742  nh_black           W           W  \n",
       "3       0.021742  nh_black           W           W  \n",
       "4       0.021742  nh_black           W           W  \n",
       "\n",
       "[5 rows x 71 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###################################################\n",
    "# Read in .txt file(s) from a specified directory #\n",
    "###################################################\n",
    "\n",
    "evals = pd.read_csv('../data/psom_evals.csv')\n",
    "evals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# Clean, lemmatize #\n",
    "####################\n",
    "w_words = ['she', 'daughter', 'hers', 'her', 'mother', 'woman', 'girl', 'herself', \n",
    "           'female', 'sister', 'daughters', 'mothers', 'women', 'girls', 'females',\n",
    "          'sisters', 'aunt', 'aunts', 'niece', 'nieces']\n",
    "m_words = ['he', 'son', 'his', 'him', 'father', 'man', 'boy', 'himself', \n",
    "           'male', 'brother', 'son', 'father', 'men', 'boys', 'males',\n",
    "          'brothers', 'uncles', 'uncle', 'nephew', 'nephews']\n",
    "\n",
    "afn = Afinn()\n",
    "\n",
    "for ID,row in evals.iterrows(): # loop through papers\n",
    "    totalWords = []\n",
    "\n",
    "    text = row['Answer.Text_Detail']\n",
    "    text = re.sub(\"\\u2013|\\u2014\", \"-\", str(text))  # Replace em-dashes\n",
    "    sents = sent_tokenize(text)  # Split into sentences\n",
    "    sents = [word_tokenize(s) for s in sents]\n",
    "    sents = [[w.translate(translator) for w in s] for s in sents]  # filter punctuation\n",
    "    sents = [[re.sub(r'^[-+]?[0-9]*[\\.\\-]?[0-9]+$', 'numeric', w) for w in s] for s in sents]  # replace all numerals with the holder \"number\"\n",
    "    sents = [[w for w in s if re.search('[^a-zA-Z-0-9-]+', w) is None] for s in sents]  # trips everything but alphanumeric\n",
    "    sents = [[w.lower() for w in s] for s in sents]  # make lower case\n",
    "    sents = [s for s in sents if len(s) > 0]  # remove empty lines\n",
    "    sents = [[w for w in s if not w in stop_words] for s in sents]  # filter stop words\n",
    "    sents = [[w for w in s if len(w) > 1] for s in sents]  # filters out variables, etc\n",
    "    sents = [[w for w in s if len(w) > 2] for s in sents]  # filters out variables, etc\n",
    "    sents = [[w for w in s if len(w) > 3] for s in sents]  # filters out variables and abbreviations\n",
    "    sents = [s for s in sents if len(s) > 0]  # remove empty lines\n",
    "    words = [[lemmatizer.lemmatize(w) for w in s if lemmatizeCondition == 1] for s in sents]  # lemmatize\n",
    "    words = list(itertools.chain.from_iterable(words))  # join list of lists\n",
    "    totalWords.append(words)\n",
    "\n",
    "    # append unique words in the whole corpus\n",
    "    my_words = list(set(list(itertools.chain.from_iterable(totalWords))))  \n",
    "    \n",
    "    # remove names\n",
    "    name = row.StudentName.lower().split(' ')\n",
    "    my_words = [w for w in my_words if w.lower() not in name]\n",
    "    words = [w for w in words if w.lower() not in name]\n",
    "    evals.loc[ID,'text_len'] = len(words)\n",
    "    \n",
    "    # sentiment analysis\n",
    "    scores = [afn.score(w) for w in words]\n",
    "\n",
    "    # loop through models\n",
    "    for i,m in enumerate(models):\n",
    "        # filter out words not in model\n",
    "        my_words = [word for word in my_words if word in m]\n",
    "        \n",
    "        # filter out words of interest\n",
    "        my_words = [word for word in my_words if word not in w_words]\n",
    "        my_words = [word for word in my_words if word not in m_words]\n",
    "        \n",
    "        # Make a list of all word-to-word distances [each as a tuple of (word1,word2,dist)]\n",
    "        sims_w = []\n",
    "        sims_m = []\n",
    "        \n",
    "        # Find similarity distances between each word pair for current year\n",
    "        for word1 in my_words:\n",
    "            sim = []\n",
    "            for word2 in w_words:\n",
    "                cosine_similarity = m.similarity(word1, word2)\n",
    "                sim.append(cosine_similarity)\n",
    "            sims_w.append(np.mean(sim))\n",
    "        for word1 in my_words:\n",
    "            sim = []\n",
    "            for word2 in m_words:\n",
    "                cosine_similarity = m.similarity(word1, word2)\n",
    "                sim.append(cosine_similarity)\n",
    "            sims_m.append(np.mean(sim))\n",
    "\n",
    "        if len(sims_w) > 0:\n",
    "            # gender coded\n",
    "            evals.loc[ID,f'mean_wom_{exts[i]}'] = np.mean(sims_w)\n",
    "            evals.loc[ID,f'med_wom_{exts[i]}'] = np.median(sims_w)\n",
    "            evals.loc[ID,f'max_wom_{exts[i]}'] = np.max(sims_w)\n",
    "            evals.loc[ID,f'mean_man_{exts[i]}'] = np.mean(sims_m)\n",
    "            evals.loc[ID,f'med_man_{exts[i]}'] = np.median(sims_m)\n",
    "            evals.loc[ID,f'max_man_{exts[i]}'] = np.max(sims_m)\n",
    "    # sentiment\n",
    "    if len(scores) > 1:\n",
    "        evals.loc[ID,'sum_sent'] = sum(scores)\n",
    "        evals.loc[ID,'mean_sent'] = np.mean(scores)\n",
    "        if sum(scores) > 0:\n",
    "            evals.loc[ID,'sent'] = 'positive'\n",
    "        else:\n",
    "            evals.loc[ID,'sent'] = 'negative'\n",
    "    else:\n",
    "        evals.loc[ID,'sum_sent'] = np.nan\n",
    "        evals.loc[ID,'mean_sent'] = np.nan\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Evaluatee</th>\n",
       "      <th>StudentName</th>\n",
       "      <th>FormID</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Question</th>\n",
       "      <th>Cid</th>\n",
       "      <th>S_URM</th>\n",
       "      <th>S_SRS.Calc.Race.Desc</th>\n",
       "      <th>Schedule.Start.Date</th>\n",
       "      <th>Answer.Text_Detail</th>\n",
       "      <th>...</th>\n",
       "      <th>max_man_bio</th>\n",
       "      <th>mean_wom_wiki</th>\n",
       "      <th>med_wom_wiki</th>\n",
       "      <th>max_wom_wiki</th>\n",
       "      <th>mean_man_wiki</th>\n",
       "      <th>med_man_wiki</th>\n",
       "      <th>max_man_wiki</th>\n",
       "      <th>sum_sent</th>\n",
       "      <th>mean_sent</th>\n",
       "      <th>sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAKHUS, ERIN</td>\n",
       "      <td>HERNANDO, MARY</td>\n",
       "      <td>e2d1755825d32c6be44b8bcf098d20f935a0a665c99a23...</td>\n",
       "      <td></td>\n",
       "      <td>Areas for improvement/Additional comments</td>\n",
       "      <td>MED</td>\n",
       "      <td>Y</td>\n",
       "      <td>Multiple ethnicities selected</td>\n",
       "      <td>9/11/2017</td>\n",
       "      <td>No concerns for her level of training. I only ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.277099</td>\n",
       "      <td>0.186815</td>\n",
       "      <td>0.180086</td>\n",
       "      <td>0.270845</td>\n",
       "      <td>0.175862</td>\n",
       "      <td>0.182334</td>\n",
       "      <td>0.231065</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAKHUS, ERIN</td>\n",
       "      <td>HERNANDO, MARY</td>\n",
       "      <td>e2d1755825d32c6be44b8bcf098d20f935a0a665c99a23...</td>\n",
       "      <td></td>\n",
       "      <td>Strengths/Summary</td>\n",
       "      <td>MED</td>\n",
       "      <td>Y</td>\n",
       "      <td>Multiple ethnicities selected</td>\n",
       "      <td>9/11/2017</td>\n",
       "      <td>Mary Ann impressed me with her thorough histor...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281561</td>\n",
       "      <td>0.189576</td>\n",
       "      <td>0.184051</td>\n",
       "      <td>0.295312</td>\n",
       "      <td>0.185006</td>\n",
       "      <td>0.181844</td>\n",
       "      <td>0.297524</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAMODT, WHITLEY</td>\n",
       "      <td>MEZOCHOW, GABRIELLE</td>\n",
       "      <td>bf3434488ad0a3be485d804cdba2d6aeee56c226519712...</td>\n",
       "      <td></td>\n",
       "      <td>Strengths/Summary</td>\n",
       "      <td>NEU</td>\n",
       "      <td>N</td>\n",
       "      <td>White, Non-Hispanic</td>\n",
       "      <td>3/26/2018</td>\n",
       "      <td>Gabby did an excellent job on the neurology se...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.315162</td>\n",
       "      <td>0.196609</td>\n",
       "      <td>0.197174</td>\n",
       "      <td>0.290263</td>\n",
       "      <td>0.199983</td>\n",
       "      <td>0.209590</td>\n",
       "      <td>0.281945</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.320755</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAMODT, WHITLEY</td>\n",
       "      <td>RECH, ANDREW</td>\n",
       "      <td>43cf5ed1d5f3a7ead23e25da8a7d5b0bee2540c12cdf3b...</td>\n",
       "      <td></td>\n",
       "      <td>Strengths/Summary</td>\n",
       "      <td>NEU</td>\n",
       "      <td>N</td>\n",
       "      <td>White, Non-Hispanic</td>\n",
       "      <td>2/26/2018</td>\n",
       "      <td>Andrew did an outstanding job on the PPMC neur...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.303533</td>\n",
       "      <td>0.192020</td>\n",
       "      <td>0.190689</td>\n",
       "      <td>0.284315</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.199738</td>\n",
       "      <td>0.311979</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.224490</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAMODT, WHITLEY</td>\n",
       "      <td>O''KEEFE, RYAN</td>\n",
       "      <td>1f637c27634cc0b947be3dfcf9998f50fb47d0185e71a4...</td>\n",
       "      <td></td>\n",
       "      <td>Strengths/Summary</td>\n",
       "      <td>NEU</td>\n",
       "      <td>N</td>\n",
       "      <td>White, Non-Hispanic</td>\n",
       "      <td>9/24/2018</td>\n",
       "      <td>Ryan did an excellent job on the HUP neurology...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.303533</td>\n",
       "      <td>0.181080</td>\n",
       "      <td>0.183204</td>\n",
       "      <td>0.280025</td>\n",
       "      <td>0.186723</td>\n",
       "      <td>0.189369</td>\n",
       "      <td>0.266286</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Evaluatee          StudentName  \\\n",
       "0     AAKHUS, ERIN       HERNANDO, MARY   \n",
       "1     AAKHUS, ERIN       HERNANDO, MARY   \n",
       "2  AAMODT, WHITLEY  MEZOCHOW, GABRIELLE   \n",
       "3  AAMODT, WHITLEY         RECH, ANDREW   \n",
       "4  AAMODT, WHITLEY       O''KEEFE, RYAN   \n",
       "\n",
       "                                              FormID Summary  \\\n",
       "0  e2d1755825d32c6be44b8bcf098d20f935a0a665c99a23...           \n",
       "1  e2d1755825d32c6be44b8bcf098d20f935a0a665c99a23...           \n",
       "2  bf3434488ad0a3be485d804cdba2d6aeee56c226519712...           \n",
       "3  43cf5ed1d5f3a7ead23e25da8a7d5b0bee2540c12cdf3b...           \n",
       "4  1f637c27634cc0b947be3dfcf9998f50fb47d0185e71a4...           \n",
       "\n",
       "                                    Question  Cid S_URM  \\\n",
       "0  Areas for improvement/Additional comments  MED     Y   \n",
       "1                          Strengths/Summary  MED     Y   \n",
       "2                          Strengths/Summary  NEU     N   \n",
       "3                          Strengths/Summary  NEU     N   \n",
       "4                          Strengths/Summary  NEU     N   \n",
       "\n",
       "            S_SRS.Calc.Race.Desc Schedule.Start.Date  \\\n",
       "0  Multiple ethnicities selected           9/11/2017   \n",
       "1  Multiple ethnicities selected           9/11/2017   \n",
       "2            White, Non-Hispanic           3/26/2018   \n",
       "3            White, Non-Hispanic           2/26/2018   \n",
       "4            White, Non-Hispanic           9/24/2018   \n",
       "\n",
       "                                  Answer.Text_Detail  ...  max_man_bio  \\\n",
       "0  No concerns for her level of training. I only ...  ...     0.277099   \n",
       "1  Mary Ann impressed me with her thorough histor...  ...     0.281561   \n",
       "2  Gabby did an excellent job on the neurology se...  ...     0.315162   \n",
       "3  Andrew did an outstanding job on the PPMC neur...  ...     0.303533   \n",
       "4  Ryan did an excellent job on the HUP neurology...  ...     0.303533   \n",
       "\n",
       "  mean_wom_wiki med_wom_wiki max_wom_wiki  mean_man_wiki  med_man_wiki  \\\n",
       "0      0.186815     0.180086     0.270845       0.175862      0.182334   \n",
       "1      0.189576     0.184051     0.295312       0.185006      0.181844   \n",
       "2      0.196609     0.197174     0.290263       0.199983      0.209590   \n",
       "3      0.192020     0.190689     0.284315       0.191886      0.199738   \n",
       "4      0.181080     0.183204     0.280025       0.186723      0.189369   \n",
       "\n",
       "   max_man_wiki  sum_sent  mean_sent      sent  \n",
       "0      0.231065       6.0   0.545455  positive  \n",
       "1      0.297524       8.0   0.242424  positive  \n",
       "2      0.281945      17.0   0.320755  positive  \n",
       "3      0.311979      11.0   0.224490  positive  \n",
       "4      0.266286       9.0   0.250000  positive  \n",
       "\n",
       "[5 rows x 87 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evals.to_csv(\"../data/psom_evals_nlp.csv\")\n",
    "evals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 57734 entries, 0 to 57733\n",
      "Data columns (total 87 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   Evaluatee             57734 non-null  object \n",
      " 1   StudentName           57734 non-null  object \n",
      " 2   FormID                57734 non-null  object \n",
      " 3   Summary               57734 non-null  object \n",
      " 4   Question              57734 non-null  object \n",
      " 5   Cid                   57734 non-null  object \n",
      " 6   S_URM                 54870 non-null  object \n",
      " 7   S_SRS.Calc.Race.Desc  55340 non-null  object \n",
      " 8   Schedule.Start.Date   57734 non-null  object \n",
      " 9   Answer.Text_Detail    56708 non-null  object \n",
      " 10  StudentID             57734 non-null  int64  \n",
      " 11  Instructor_ID         46854 non-null  object \n",
      " 12  F_Gender              24762 non-null  object \n",
      " 13  F_Race_Ethnicity      24762 non-null  object \n",
      " 14  F_Afr.Amer            24762 non-null  float64\n",
      " 15  F_Asian               24762 non-null  float64\n",
      " 16  F_Amer.Ind            24762 non-null  float64\n",
      " 17  F_Hisp.Lat            24762 non-null  float64\n",
      " 18  F_Pac.Isl             24762 non-null  float64\n",
      " 19  F_White               24762 non-null  float64\n",
      " 20  uniqueid              57734 non-null  object \n",
      " 21  first.x               57734 non-null  object \n",
      " 22  prob.w                57734 non-null  float64\n",
      " 23  prob.m                57734 non-null  float64\n",
      " 24  last.x                57734 non-null  object \n",
      " 25  X__name.x             57734 non-null  object \n",
      " 26  rowindex.x            57734 non-null  int64  \n",
      " 27  asian_mean            57734 non-null  float64\n",
      " 28  asian_std             57734 non-null  float64\n",
      " 29  asian_lb              57734 non-null  float64\n",
      " 30  asian_ub              57734 non-null  float64\n",
      " 31  hispanic_mean         57734 non-null  float64\n",
      " 32  hispanic_std          57734 non-null  float64\n",
      " 33  hispanic_lb           57734 non-null  float64\n",
      " 34  hispanic_ub           57734 non-null  float64\n",
      " 35  nh_black_mean         57734 non-null  float64\n",
      " 36  nh_black_std          57734 non-null  float64\n",
      " 37  nh_black_lb           57734 non-null  float64\n",
      " 38  nh_black_ub           57734 non-null  float64\n",
      " 39  nh_white_mean         57734 non-null  float64\n",
      " 40  nh_white_std          57734 non-null  float64\n",
      " 41  nh_white_lb           57734 non-null  float64\n",
      " 42  nh_white_ub           57734 non-null  float64\n",
      " 43  race                  57734 non-null  object \n",
      " 44  gender50              57128 non-null  object \n",
      " 45  gender70              54274 non-null  object \n",
      " 46  first.y               57734 non-null  object \n",
      " 47  F_prob.w              57734 non-null  float64\n",
      " 48  F_prob.m              57734 non-null  float64\n",
      " 49  last.y                57734 non-null  object \n",
      " 50  X__name.y             57734 non-null  object \n",
      " 51  rowindex.y            57734 non-null  int64  \n",
      " 52  F_asian_mean          57734 non-null  float64\n",
      " 53  F_asian_std           57734 non-null  float64\n",
      " 54  F_asian_lb            57734 non-null  float64\n",
      " 55  F_asian_ub            57734 non-null  float64\n",
      " 56  F_hispanic_mean       57734 non-null  float64\n",
      " 57  F_hispanic_std        57734 non-null  float64\n",
      " 58  F_hispanic_lb         57734 non-null  float64\n",
      " 59  F_hispanic_ub         57734 non-null  float64\n",
      " 60  F_nh_black_mean       57734 non-null  float64\n",
      " 61  F_nh_black_std        57734 non-null  float64\n",
      " 62  F_nh_black_lb         57734 non-null  float64\n",
      " 63  F_nh_black_ub         57734 non-null  float64\n",
      " 64  F_nh_white_mean       57734 non-null  float64\n",
      " 65  F_nh_white_std        57734 non-null  float64\n",
      " 66  F_nh_white_lb         57734 non-null  float64\n",
      " 67  F_nh_white_ub         57734 non-null  float64\n",
      " 68  F_race                57734 non-null  object \n",
      " 69  F_gender50            46454 non-null  object \n",
      " 70  F_gender70            44052 non-null  object \n",
      " 71  text_len              57734 non-null  float64\n",
      " 72  mean_wom_bio          56248 non-null  float64\n",
      " 73  med_wom_bio           56248 non-null  float64\n",
      " 74  max_wom_bio           56248 non-null  float64\n",
      " 75  mean_man_bio          56248 non-null  float64\n",
      " 76  med_man_bio           56248 non-null  float64\n",
      " 77  max_man_bio           56248 non-null  float64\n",
      " 78  mean_wom_wiki         56246 non-null  float64\n",
      " 79  med_wom_wiki          56246 non-null  float64\n",
      " 80  max_wom_wiki          56246 non-null  float64\n",
      " 81  mean_man_wiki         56246 non-null  float64\n",
      " 82  med_man_wiki          56246 non-null  float64\n",
      " 83  max_man_wiki          56246 non-null  float64\n",
      " 84  sum_sent              54685 non-null  float64\n",
      " 85  mean_sent             54685 non-null  float64\n",
      " 86  sent                  54685 non-null  object \n",
      "dtypes: float64(57), int64(3), object(27)\n",
      "memory usage: 38.3+ MB\n"
     ]
    }
   ],
   "source": [
    "evals.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psom",
   "language": "python",
   "name": "psom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
